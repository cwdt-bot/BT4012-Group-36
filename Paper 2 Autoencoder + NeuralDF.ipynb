{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Paper 2: Autoencoder + NeuralDF.ipynb","provenance":[],"collapsed_sections":["bUOGJIzZd-HE","JTBrSYOnnKNk","_x94PCn6nTX4","ZxrYWQG6RbLq"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UTbWMnwyWtT-"},"source":["# Downgrade tensorflow to V1"]},{"cell_type":"code","metadata":{"id":"YJr5_54uWrJC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636638899294,"user_tz":-480,"elapsed":51811,"user":{"displayName":"Darren Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimF-x90QeCVG9qbvb6LswazUwqnFzwVWmAytJfFQ=s64","userId":"11292448669415810479"}},"outputId":"77a4e1d0-21e3-4a49-eb38-7e269397701f"},"source":["! pip install tensorflow-gpu==1.15"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-gpu==1.15\n","  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n","\u001b[K     |████████████████████████████████| 411.5 MB 7.8 kB/s \n","\u001b[?25hCollecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.17.3)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.37.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.13.3)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.41.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.12.0)\n","Collecting tensorflow-estimator==1.15.1\n","  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n","\u001b[K     |████████████████████████████████| 503 kB 33.7 MB/s \n","\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n","  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 33.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.19.5)\n","Collecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (3.1.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.4)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.8.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15) (1.5.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.6.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=137da57f533ffd849b19f888aa23c9c9345b6dfa639b9e642ececa07431d5463\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.7.0\n","    Uninstalling tensorflow-estimator-2.7.0:\n","      Successfully uninstalled tensorflow-estimator-2.7.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.7.0\n","    Uninstalling tensorboard-2.7.0:\n","      Successfully uninstalled tensorboard-2.7.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n","tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\n","tensorflow-probability 0.14.1 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"bUOGJIzZd-HE"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"SL6Rl80-nPjj","executionInfo":{"status":"ok","timestamp":1636638908475,"user_tz":-480,"elapsed":1811,"user":{"displayName":"Darren Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimF-x90QeCVG9qbvb6LswazUwqnFzwVWmAytJfFQ=s64","userId":"11292448669415810479"}}},"source":["from __future__ import division, print_function, absolute_import\n","import pandas as pd\n","import pickle\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","import numpy as np\n","from sklearn import preprocessing\n","import os\n","import time\n","\n","import tensorflow as tf\n","\n","from xgboost import XGBClassifier\n","import datetime\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kM23WkkoUct-"},"source":["# Loading Functions"]},{"cell_type":"code","metadata":{"id":"N0PjVxlrUeuA"},"source":["def unpickle(file):\n","    import pickle\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo,encoding='bytes')\n","    return dict\n","\n","\n","def reconstructlabel(label):\n","    length = len(label)\n","    newlabel = []\n","    for index in range(length):\n","        if label[index] == 0:\n","            newlabel.append([0, 1])\n","        else:\n","            newlabel.append([1, 0])\n","    return newlabel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTBrSYOnnKNk"},"source":["# Download Datasets"]},{"cell_type":"markdown","metadata":{"id":"3YkF47IEu0BQ"},"source":["## Amazon Opinion Fraud (Used in Paper)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqJuTaCru43Q","executionInfo":{"status":"ok","timestamp":1636462151214,"user_tz":-480,"elapsed":4216,"user":{"displayName":"Badly taught","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11394682269253795404"}},"outputId":"e8acdf6f-4802-4fa8-a04e-74e3354caf4e"},"source":["# Amazon\n","! gdown --id 17leD629H6QsruNiNKYMw1k-yhW3sbeKM"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=17leD629H6QsruNiNKYMw1k-yhW3sbeKM\n","To: /content/dataset.p\n","100% 31.2M/31.2M [00:00<00:00, 145MB/s]\n"]}]},{"cell_type":"code","metadata":{"id":"K71npKE5u9SQ"},"source":["am_data = unpickle('dataset.p')\n","\n","body = am_data[1:,:]\n","arr_am_X = body[:,1:]\n","arr_am_Y = body[:,0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCnxwgGOOkwI"},"source":["## Ethereum Fraud"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cL29nhNRt4QP","executionInfo":{"status":"ok","timestamp":1636462153764,"user_tz":-480,"elapsed":2556,"user":{"displayName":"Badly taught","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11394682269253795404"}},"outputId":"0a1cd7f6-88cf-4060-c4f9-406078fef914"},"source":["# Ethereum\n","# https://www.kaggle.com/vagifa/ethereum-frauddetection-dataset\n","! gdown --id 1WF37JnENj_Y3Mu661DYFotCiGAo2XB-7"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1WF37JnENj_Y3Mu661DYFotCiGAo2XB-7\n","To: /content/etherum_transaction_dataset.csv\n","\r  0% 0.00/2.88M [00:00<?, ?B/s]\r100% 2.88M/2.88M [00:00<00:00, 91.9MB/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceu09Ir4-lG_","executionInfo":{"status":"ok","timestamp":1636462153766,"user_tz":-480,"elapsed":16,"user":{"displayName":"Badly taught","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11394682269253795404"}},"outputId":"3fc0319c-c470-40bf-b60b-5629f897e7bc"},"source":["df_eth = pd.read_csv('etherum_transaction_dataset.csv')\n","\n","df_eth.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Index</th>\n","      <th>Address</th>\n","      <th>FLAG</th>\n","      <th>Avg min between sent tnx</th>\n","      <th>Avg min between received tnx</th>\n","      <th>Time Diff between first and last (Mins)</th>\n","      <th>Sent tnx</th>\n","      <th>Received Tnx</th>\n","      <th>Number of Created Contracts</th>\n","      <th>Unique Received From Addresses</th>\n","      <th>Unique Sent To Addresses</th>\n","      <th>min value received</th>\n","      <th>max value received</th>\n","      <th>avg val received</th>\n","      <th>min val sent</th>\n","      <th>max val sent</th>\n","      <th>avg val sent</th>\n","      <th>min value sent to contract</th>\n","      <th>max val sent to contract</th>\n","      <th>avg value sent to contract</th>\n","      <th>total transactions (including tnx to create contract</th>\n","      <th>total Ether sent</th>\n","      <th>total ether received</th>\n","      <th>total ether sent contracts</th>\n","      <th>total ether balance</th>\n","      <th>Total ERC20 tnxs</th>\n","      <th>ERC20 total Ether received</th>\n","      <th>ERC20 total ether sent</th>\n","      <th>ERC20 total Ether sent contract</th>\n","      <th>ERC20 uniq sent addr</th>\n","      <th>ERC20 uniq rec addr</th>\n","      <th>ERC20 uniq sent addr.1</th>\n","      <th>ERC20 uniq rec contract addr</th>\n","      <th>ERC20 avg time between sent tnx</th>\n","      <th>ERC20 avg time between rec tnx</th>\n","      <th>ERC20 avg time between rec 2 tnx</th>\n","      <th>ERC20 avg time between contract tnx</th>\n","      <th>ERC20 min val rec</th>\n","      <th>ERC20 max val rec</th>\n","      <th>ERC20 avg val rec</th>\n","      <th>ERC20 min val sent</th>\n","      <th>ERC20 max val sent</th>\n","      <th>ERC20 avg val sent</th>\n","      <th>ERC20 min val sent contract</th>\n","      <th>ERC20 max val sent contract</th>\n","      <th>ERC20 avg val sent contract</th>\n","      <th>ERC20 uniq sent token name</th>\n","      <th>ERC20 uniq rec token name</th>\n","      <th>ERC20 most sent token type</th>\n","      <th>ERC20_most_rec_token_type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0x00009277775ac7d0d59eaad8fee3d10ac6c805e8</td>\n","      <td>0</td>\n","      <td>844.26</td>\n","      <td>1093.71</td>\n","      <td>704785.63</td>\n","      <td>721</td>\n","      <td>89</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>118</td>\n","      <td>0.000000</td>\n","      <td>45.806785</td>\n","      <td>6.589513</td>\n","      <td>0.00</td>\n","      <td>31.220000</td>\n","      <td>1.200681</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>810</td>\n","      <td>865.691093</td>\n","      <td>586.466675</td>\n","      <td>0.0</td>\n","      <td>-279.224419</td>\n","      <td>265.0</td>\n","      <td>3.558854e+07</td>\n","      <td>3.560317e+07</td>\n","      <td>0.0</td>\n","      <td>30.0</td>\n","      <td>54.0</td>\n","      <td>0.0</td>\n","      <td>58.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.500000e+07</td>\n","      <td>265586.147600</td>\n","      <td>0.000000</td>\n","      <td>1.683100e+07</td>\n","      <td>271779.920000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>39.0</td>\n","      <td>57.0</td>\n","      <td>Cofoundit</td>\n","      <td>Numeraire</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0x0002b44ddb1476db43c868bd494422ee4c136fed</td>\n","      <td>0</td>\n","      <td>12709.07</td>\n","      <td>2958.44</td>\n","      <td>1218216.73</td>\n","      <td>94</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>14</td>\n","      <td>0.000000</td>\n","      <td>2.613269</td>\n","      <td>0.385685</td>\n","      <td>0.00</td>\n","      <td>1.800000</td>\n","      <td>0.032844</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>102</td>\n","      <td>3.087297</td>\n","      <td>3.085478</td>\n","      <td>0.0</td>\n","      <td>-0.001819</td>\n","      <td>8.0</td>\n","      <td>4.034283e+02</td>\n","      <td>2.260809e+00</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.650000e+02</td>\n","      <td>57.632615</td>\n","      <td>2.260809</td>\n","      <td>2.260809e+00</td>\n","      <td>2.260809</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>Livepeer Token</td>\n","      <td>Livepeer Token</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0x0002bda54cb772d040f779e88eb453cac0daa244</td>\n","      <td>0</td>\n","      <td>246194.54</td>\n","      <td>2434.02</td>\n","      <td>516729.30</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>0.113119</td>\n","      <td>1.165453</td>\n","      <td>0.358906</td>\n","      <td>0.05</td>\n","      <td>3.538616</td>\n","      <td>1.794308</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>12</td>\n","      <td>3.588616</td>\n","      <td>3.589057</td>\n","      <td>0.0</td>\n","      <td>0.000441</td>\n","      <td>8.0</td>\n","      <td>5.215121e+02</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.428198e+02</td>\n","      <td>65.189009</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>None</td>\n","      <td>XENON</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0x00038e6ba2fd5c09aedb96697c8d7b8fa6632e5e</td>\n","      <td>0</td>\n","      <td>10219.60</td>\n","      <td>15785.09</td>\n","      <td>397555.90</td>\n","      <td>25</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>13</td>\n","      <td>0.000000</td>\n","      <td>500.000000</td>\n","      <td>99.488840</td>\n","      <td>0.00</td>\n","      <td>450.000000</td>\n","      <td>70.001834</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>34</td>\n","      <td>1750.045862</td>\n","      <td>895.399559</td>\n","      <td>0.0</td>\n","      <td>-854.646303</td>\n","      <td>14.0</td>\n","      <td>1.711105e+04</td>\n","      <td>1.141223e+04</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>11.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.141223e+04</td>\n","      <td>1555.550174</td>\n","      <td>100.000000</td>\n","      <td>9.029231e+03</td>\n","      <td>3804.076893</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>11.0</td>\n","      <td>Raiden</td>\n","      <td>XENON</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>0x00062d1dd1afb6fb02540ddad9cdebfe568e0d89</td>\n","      <td>0</td>\n","      <td>36.61</td>\n","      <td>10707.77</td>\n","      <td>382472.42</td>\n","      <td>4598</td>\n","      <td>20</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>19</td>\n","      <td>0.000000</td>\n","      <td>12.802411</td>\n","      <td>2.671095</td>\n","      <td>0.00</td>\n","      <td>9.000000</td>\n","      <td>0.022688</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4619</td>\n","      <td>104.318883</td>\n","      <td>53.421896</td>\n","      <td>0.0</td>\n","      <td>-50.896986</td>\n","      <td>42.0</td>\n","      <td>1.628297e+05</td>\n","      <td>1.235399e+05</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>23.0</td>\n","      <td>0.0</td>\n","      <td>27.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.000000e+04</td>\n","      <td>4934.232147</td>\n","      <td>0.000000</td>\n","      <td>4.500000e+04</td>\n","      <td>13726.659220</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>27.0</td>\n","      <td>StatusNetwork</td>\n","      <td>EOS</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  Index  ...  ERC20 most sent token type   ERC20_most_rec_token_type\n","0           0      1  ...                   Cofoundit                   Numeraire\n","1           1      2  ...              Livepeer Token              Livepeer Token\n","2           2      3  ...                        None                       XENON\n","3           3      4  ...                      Raiden                       XENON\n","4           4      5  ...               StatusNetwork                         EOS\n","\n","[5 rows x 51 columns]"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abehUpf3PRgP","executionInfo":{"status":"ok","timestamp":1636462153767,"user_tz":-480,"elapsed":13,"user":{"displayName":"Badly taught","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11394682269253795404"}},"outputId":"57e74897-dc66-4dd2-9171-669a721930e8"},"source":["df_eth.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Index</th>\n","      <th>FLAG</th>\n","      <th>Avg min between sent tnx</th>\n","      <th>Avg min between received tnx</th>\n","      <th>Time Diff between first and last (Mins)</th>\n","      <th>Sent tnx</th>\n","      <th>Received Tnx</th>\n","      <th>Number of Created Contracts</th>\n","      <th>Unique Received From Addresses</th>\n","      <th>Unique Sent To Addresses</th>\n","      <th>min value received</th>\n","      <th>max value received</th>\n","      <th>avg val received</th>\n","      <th>min val sent</th>\n","      <th>max val sent</th>\n","      <th>avg val sent</th>\n","      <th>min value sent to contract</th>\n","      <th>max val sent to contract</th>\n","      <th>avg value sent to contract</th>\n","      <th>total transactions (including tnx to create contract</th>\n","      <th>total Ether sent</th>\n","      <th>total ether received</th>\n","      <th>total ether sent contracts</th>\n","      <th>total ether balance</th>\n","      <th>Total ERC20 tnxs</th>\n","      <th>ERC20 total Ether received</th>\n","      <th>ERC20 total ether sent</th>\n","      <th>ERC20 total Ether sent contract</th>\n","      <th>ERC20 uniq sent addr</th>\n","      <th>ERC20 uniq rec addr</th>\n","      <th>ERC20 uniq sent addr.1</th>\n","      <th>ERC20 uniq rec contract addr</th>\n","      <th>ERC20 avg time between sent tnx</th>\n","      <th>ERC20 avg time between rec tnx</th>\n","      <th>ERC20 avg time between rec 2 tnx</th>\n","      <th>ERC20 avg time between contract tnx</th>\n","      <th>ERC20 min val rec</th>\n","      <th>ERC20 max val rec</th>\n","      <th>ERC20 avg val rec</th>\n","      <th>ERC20 min val sent</th>\n","      <th>ERC20 max val sent</th>\n","      <th>ERC20 avg val sent</th>\n","      <th>ERC20 min val sent contract</th>\n","      <th>ERC20 max val sent contract</th>\n","      <th>ERC20 avg val sent contract</th>\n","      <th>ERC20 uniq sent token name</th>\n","      <th>ERC20 uniq rec token name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9.841000e+03</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9841.000000</td>\n","      <td>9.841000e+03</td>\n","      <td>9.841000e+03</td>\n","      <td>9841.000000</td>\n","      <td>9.841000e+03</td>\n","      <td>9012.000000</td>\n","      <td>9.012000e+03</td>\n","      <td>9.012000e+03</td>\n","      <td>9012.000000</td>\n","      <td>9012.000000</td>\n","      <td>9012.000000</td>\n","      <td>9012.000000</td>\n","      <td>9012.000000</td>\n","      <td>9012.0</td>\n","      <td>9012.0</td>\n","      <td>9012.0</td>\n","      <td>9012.0</td>\n","      <td>9012.000000</td>\n","      <td>9.012000e+03</td>\n","      <td>9.012000e+03</td>\n","      <td>9.012000e+03</td>\n","      <td>9.012000e+03</td>\n","      <td>9.012000e+03</td>\n","      <td>9012.0</td>\n","      <td>9012.0</td>\n","      <td>9012.0</td>\n","      <td>9012.000000</td>\n","      <td>9012.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>4920.000000</td>\n","      <td>1815.049893</td>\n","      <td>0.221421</td>\n","      <td>5086.878721</td>\n","      <td>8004.851184</td>\n","      <td>2.183333e+05</td>\n","      <td>115.931714</td>\n","      <td>163.700945</td>\n","      <td>3.729702</td>\n","      <td>30.360939</td>\n","      <td>25.840159</td>\n","      <td>43.845153</td>\n","      <td>523.152481</td>\n","      <td>100.711721</td>\n","      <td>4.800090</td>\n","      <td>314.617297</td>\n","      <td>44.755731</td>\n","      <td>0.000003</td>\n","      <td>0.000008</td>\n","      <td>0.000005</td>\n","      <td>283.362362</td>\n","      <td>1.016092e+04</td>\n","      <td>1.163832e+04</td>\n","      <td>0.000008</td>\n","      <td>1.477395e+03</td>\n","      <td>36.255659</td>\n","      <td>1.296207e+08</td>\n","      <td>1.386849e+07</td>\n","      <td>110.939207</td>\n","      <td>5.638038</td>\n","      <td>7.598535</td>\n","      <td>0.003440</td>\n","      <td>4.901909</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>485.614688</td>\n","      <td>1.252524e+08</td>\n","      <td>4.346203e+06</td>\n","      <td>1.174126e+04</td>\n","      <td>1.303594e+07</td>\n","      <td>6.318389e+06</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.384931</td>\n","      <td>4.826676</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>2840.996333</td>\n","      <td>1222.621830</td>\n","      <td>0.415224</td>\n","      <td>21486.549974</td>\n","      <td>23081.714801</td>\n","      <td>3.229379e+05</td>\n","      <td>757.226361</td>\n","      <td>940.836550</td>\n","      <td>141.445583</td>\n","      <td>298.621112</td>\n","      <td>263.820410</td>\n","      <td>325.929139</td>\n","      <td>13008.821539</td>\n","      <td>2885.002236</td>\n","      <td>138.609682</td>\n","      <td>6629.212643</td>\n","      <td>239.080215</td>\n","      <td>0.000225</td>\n","      <td>0.000516</td>\n","      <td>0.000323</td>\n","      <td>1352.404013</td>\n","      <td>3.583227e+05</td>\n","      <td>3.642048e+05</td>\n","      <td>0.000516</td>\n","      <td>2.424254e+05</td>\n","      <td>447.528908</td>\n","      <td>1.053858e+10</td>\n","      <td>1.180390e+09</td>\n","      <td>6128.634953</td>\n","      <td>105.252500</td>\n","      <td>81.818470</td>\n","      <td>0.065698</td>\n","      <td>17.246576</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>16883.278712</td>\n","      <td>1.053741e+10</td>\n","      <td>2.141192e+08</td>\n","      <td>1.053567e+06</td>\n","      <td>1.179905e+09</td>\n","      <td>5.914764e+08</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.735121</td>\n","      <td>16.678607</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>-1.560535e+07</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>2460.000000</td>\n","      <td>821.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>3.169300e+02</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.001000</td>\n","      <td>1.000000</td>\n","      <td>0.426905</td>\n","      <td>0.000000</td>\n","      <td>0.164577</td>\n","      <td>0.086184</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>4.000000</td>\n","      <td>2.262059e-01</td>\n","      <td>2.670424e+00</td>\n","      <td>0.000000</td>\n","      <td>6.214900e-04</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>4920.000000</td>\n","      <td>1641.000000</td>\n","      <td>0.000000</td>\n","      <td>17.340000</td>\n","      <td>509.770000</td>\n","      <td>4.663703e+04</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>0.095856</td>\n","      <td>6.000000</td>\n","      <td>1.729730</td>\n","      <td>0.049126</td>\n","      <td>4.999380</td>\n","      <td>1.606000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>8.000000</td>\n","      <td>1.248680e+01</td>\n","      <td>3.052963e+01</td>\n","      <td>0.000000</td>\n","      <td>1.722000e-03</td>\n","      <td>1.000000</td>\n","      <td>1.000000e-12</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>7380.000000</td>\n","      <td>2601.000000</td>\n","      <td>0.000000</td>\n","      <td>565.470000</td>\n","      <td>5480.390000</td>\n","      <td>3.040710e+05</td>\n","      <td>11.000000</td>\n","      <td>27.000000</td>\n","      <td>0.000000</td>\n","      <td>5.000000</td>\n","      <td>3.000000</td>\n","      <td>2.000000</td>\n","      <td>67.067040</td>\n","      <td>22.000000</td>\n","      <td>0.998800</td>\n","      <td>61.520653</td>\n","      <td>21.999380</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>54.000000</td>\n","      <td>1.009990e+02</td>\n","      <td>1.010000e+02</td>\n","      <td>0.000000</td>\n","      <td>4.452000e-02</td>\n","      <td>3.000000</td>\n","      <td>1.003370e+02</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.001523</td>\n","      <td>9.900000e+01</td>\n","      <td>2.946467e+01</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>9840.000000</td>\n","      <td>4729.000000</td>\n","      <td>1.000000</td>\n","      <td>430287.670000</td>\n","      <td>482175.490000</td>\n","      <td>1.954861e+06</td>\n","      <td>10000.000000</td>\n","      <td>10000.000000</td>\n","      <td>9995.000000</td>\n","      <td>9999.000000</td>\n","      <td>9287.000000</td>\n","      <td>10000.000000</td>\n","      <td>800000.000000</td>\n","      <td>283618.831600</td>\n","      <td>12000.000000</td>\n","      <td>520000.000000</td>\n","      <td>12000.000000</td>\n","      <td>0.020000</td>\n","      <td>0.046029</td>\n","      <td>0.023014</td>\n","      <td>19995.000000</td>\n","      <td>2.858096e+07</td>\n","      <td>2.858159e+07</td>\n","      <td>0.046029</td>\n","      <td>1.428864e+07</td>\n","      <td>10001.000000</td>\n","      <td>1.000020e+12</td>\n","      <td>1.120000e+11</td>\n","      <td>416000.000000</td>\n","      <td>6582.000000</td>\n","      <td>4293.000000</td>\n","      <td>3.000000</td>\n","      <td>782.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>990000.000000</td>\n","      <td>1.000000e+12</td>\n","      <td>1.724181e+10</td>\n","      <td>1.000000e+08</td>\n","      <td>1.120000e+11</td>\n","      <td>5.614756e+10</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>213.000000</td>\n","      <td>737.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Unnamed: 0  ...   ERC20 uniq rec token name\n","count  9841.000000  ...                 9012.000000\n","mean   4920.000000  ...                    4.826676\n","std    2840.996333  ...                   16.678607\n","min       0.000000  ...                    0.000000\n","25%    2460.000000  ...                    0.000000\n","50%    4920.000000  ...                    1.000000\n","75%    7380.000000  ...                    2.000000\n","max    9840.000000  ...                  737.000000\n","\n","[8 rows x 48 columns]"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"_x94PCn6nTX4"},"source":["# Data Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"5EnJ1TnVuuOC"},"source":["## Amazon Opinion Fraud"]},{"cell_type":"code","metadata":{"id":"6VhASgoKuxtG"},"source":["arr_am_X_norm=preprocessing.scale(arr_am_X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pqaAh1F_xIf1"},"source":["# train-val split\n","\n","x_train_am, x_val_am, y_train_am, y_val_am = train_test_split(arr_am_X_norm, arr_am_Y.astype(int), test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQKHSy6gW_Dy"},"source":["y_train_am_rl = np.array(reconstructlabel(y_train_am))\n","y_val_am_rl = np.array(reconstructlabel(y_val_am))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MgchQQLQQqp5"},"source":["## Ethereum Fraud"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bi56vNRI_GVT","executionInfo":{"status":"ok","timestamp":1636462155387,"user_tz":-480,"elapsed":8,"user":{"displayName":"Badly taught","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11394682269253795404"}},"outputId":"e791eb22-fef6-4453-eb84-36dea421a49e"},"source":["# Get X variables only + drop last 2 categorical variables - poor model performance with one-hot\n","\n","df_eth_X = df_eth.drop(['Unnamed: 0', 'Index', 'Address', 'FLAG', df_eth.columns[-2], df_eth.columns[-1]], axis = 1)\n","\n","# Fix NAN\n","df_eth_X = df_eth_X.fillna(0)\n","\n","# Scale continuous variables\n","#min_max_scaler = preprocessing.MinMaxScaler()\n","#x_scaled = min_max_scaler.fit_transform(df_eth_X.values)\n","#df_eth_X = pd.DataFrame(x_scaled, columns = df_eth_X.columns)\n","#df_eth_X.head()\n","df_eth_X_norm=preprocessing.scale(df_eth_X)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\"Numerical issues were encountered \"\n"]}]},{"cell_type":"code","metadata":{"id":"gXHYIBnmAa57"},"source":["x_train_eth, x_val_eth, y_train_eth, y_val_eth = train_test_split(df_eth_X_norm, df_eth['FLAG'], test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ulrJoEwXInw"},"source":["y_train_eth_rl = np.array(reconstructlabel(y_train_eth.to_list()))\n","y_val_eth_rl = np.array(reconstructlabel(y_val_eth.to_list()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7_iqUTQ2bslq"},"source":["# Model Parameters"]},{"cell_type":"code","metadata":{"id":"6ePmJmj5bu81"},"source":["# Parameter Settings\n","DEPTH   = 3                 # Depth of a tree\n","N_LEAF  = 2 ** (DEPTH + 1)  # Number of leaf node \n","N_LABEL = 2                # Number of classes\n","N_TREE  = 5                 # Number of trees (ensemble)\n","N_BATCH = 50             # Number of data points per mini-batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gV_GCwVX3Ap6"},"source":["# Model Functions"]},{"cell_type":"markdown","metadata":{"id":"Q8K_ZgLLUSfH"},"source":["## Utility Functions"]},{"cell_type":"code","metadata":{"id":"_zJB-EfAUOC8"},"source":["# ======================functions================================ #\n","\n","def weight_variable(shape):\n","    initial = tf.truncated_normal(shape, stddev=0.1)\n","    return tf.Variable(initial)\n","\n","\n","def bias_variable(shape):\n","    initial = tf.constant(0.1, shape=shape)\n","    return tf.Variable(initial)\n","\n","\n","# constructing neural random forest\n","def model(input_layer, w_t_e, w_d_e, w_l_e, p_keep_hidden):\n","    assert (len(w_t_e) == len(w_d_e))\n","    assert (len(w_t_e) == len(w_l_e))\n","    decision_p_e = []\n","    leaf_p_e = []\n","    for w_t, w_d, w_l in zip(w_t_e, w_d_e, w_l_e):\n","        tree_layer = tf.nn.relu(tf.matmul(input_layer, w_t))\n","        tree_layer = tf.nn.dropout(tree_layer, p_keep_hidden)\n","\n","        decision_p = tf.nn.sigmoid(tf.matmul(tree_layer, w_d)) \n","        leaf_p = tf.nn.softmax(w_l) \n","\n","        decision_p_e.append(decision_p)\n","        leaf_p_e.append(leaf_p)\n","    return decision_p_e, leaf_p_e\n","\n","def neural_random_forest(decision_p_e, leaf_p_e, n_depth, n_leaf, n_batch):\n","    flat_decision_p_e = []\n","\n","    for decision_p in decision_p_e:\n","        decision_p_comp = tf.subtract(tf.ones_like(decision_p), decision_p)\n","        decision_p_pack = tf.stack([decision_p, decision_p_comp])\n","        flat_decision_p = tf.reshape(decision_p_pack, [-1])\n","        flat_decision_p_e.append(flat_decision_p)\n","\n","    batch_0_indices = \\\n","        tf.tile(tf.expand_dims(tf.range(0, n_batch * n_leaf, n_leaf), 1), [1, n_leaf])\n","\n","    # The routing probability computation\n","    in_repeat = int(n_leaf / 2)\n","    out_repeat = n_batch\n","\n","    batch_complement_indices = \\\n","        np.array([[0] * in_repeat, [n_batch * n_leaf] * in_repeat]\n","                 * out_repeat).reshape(n_batch, n_leaf)\n","\n","    mu_e = []\n","    # iterate over each tree\n","    for i, flat_decision_p in enumerate(flat_decision_p_e):\n","        mu = tf.gather(flat_decision_p, tf.add(batch_0_indices, batch_complement_indices))\n","        mu_e.append(mu)\n","\n","    # from the second layer to the last layer, we make the decision nodes\n","    for d in range(1, n_depth + 1):\n","        indices = tf.range(2 ** d, 2 ** (d + 1)) - 1\n","        tile_indices = tf.reshape(tf.tile(tf.expand_dims(indices, 1),\n","                                          [1, 2 ** (n_depth - d + 1)]), [1, -1])\n","        batch_indices = tf.add(batch_0_indices, tf.tile(tile_indices, [n_batch, 1]))\n","\n","        in_repeat = int(in_repeat / 2)\n","        out_repeat = int(out_repeat * 2)\n","\n","        batch_complement_indices = \\\n","            np.array([[0] * in_repeat, [n_batch * n_leaf] * in_repeat]\n","                     * out_repeat).reshape(n_batch, n_leaf)\n","\n","        mu_e_update = []\n","        for mu, flat_decision_p in zip(mu_e, flat_decision_p_e):\n","            mu = tf.multiply(mu, tf.gather(flat_decision_p, tf.add(batch_indices, batch_complement_indices)))\n","            mu_e_update.append(mu)\n","\n","        mu_e = mu_e_update\n","\n","    return mu_e\n","\n","\n","def probability_y_x(mu_e, leaf_p_e):\n","    py_x_e = []\n","    py_x_leaf_e = []\n","    for mu, leaf_p in zip(mu_e, leaf_p_e):\n","        # average all the leaf p\n","        py_x_tree = tf.reduce_sum(\n","            tf.multiply(tf.tile(tf.expand_dims(mu, 2), [1, 1, N_LABEL]),\n","                        tf.tile(tf.expand_dims(leaf_p, 0), [N_BATCH, 1, 1])), 1)\n","        a_tree = tf.multiply(tf.tile(tf.expand_dims(mu, 2), [1, 1, N_LABEL]),\n","                             tf.tile(tf.expand_dims(leaf_p, 0), [N_BATCH, 1, 1]))\n","        py_x_e.append(py_x_tree)\n","        py_x_leaf_e.append(a_tree)\n","\n","    py_x_e = tf.stack(py_x_e)\n","    py_x = tf.reduce_sum(py_x_e, 0)\n","    final = tf.nn.softmax(py_x)\n","    # py_x_leaf_e = tf.stack(py_x_leaf_e)\n","    return final\n","\n","\n","def init_prob_weights(shape, minval=-5, maxval=5):\n","    return tf.Variable(tf.random_uniform(shape, minval, maxval))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CSgSNoWbRc2I"},"source":["## Actual Models"]},{"cell_type":"markdown","metadata":{"id":"4hNab8LsT3Ep"},"source":["### Autoencoder + Decision Forest"]},{"cell_type":"markdown","metadata":{"id":"ZrM50ukrT7oz"},"source":["Taken from the paper's author and modified (ensure that we are using the same loss function, optimizer network structure and other hyperparameters:\n","\n","https://github.com/dongmanqing/Opinion-Fraud-Detection-via-Neural-Autoencoder-Decision-Forest/blob/master/main.py\n","\n","Uses sigmoid activation for autoencoder"]},{"cell_type":"code","metadata":{"id":"yzKxzkJkT6A0"},"source":["def run_model(N_feature, trX, teX, trY, teY, num_epoch, early_stop):\n","\n","\n","    # Input X, output Y\n","    x = tf.placeholder(\"float\", [N_BATCH, N_feature])\n","    y = tf.placeholder(\"float\", [N_BATCH, N_LABEL])\n","\n","    p_keep_conv = tf.placeholder(\"float\")\n","    p_keep_hidden = tf.placeholder(\"float\")\n","\n","    # ====================Autoencoder========================================#\n","    num_hidden_1 = 64  \n","    num_hidden_2 = 96  \n","    # num_hidden_3 = 64\n","    # num_hidden_4 = 96\n","    num_input = N_feature  \n","\n","    weights = {\n","        'encoder_h1': tf.Variable(tf.truncated_normal([num_input, num_hidden_1], stddev=0.1)),\n","        'encoder_h2': tf.Variable(tf.truncated_normal([num_hidden_1, num_hidden_2], stddev=0.1)),\n","        # 'encoder_h3': tf.Variable(tf.truncated_normal([num_hidden_2, num_hidden_3], stddev=0.1)),\n","        # 'encoder_h4': tf.Variable(tf.truncated_normal([num_hidden_3, num_hidden_4], stddev=0.1)),\n","        # 'decoder_h1': tf.Variable(tf.truncated_normal([num_hidden_4, num_hidden_3], stddev=0.1)),\n","        # 'decoder_h2': tf.Variable(tf.truncated_normal([num_hidden_3, num_hidden_2], stddev=0.1)),\n","        'decoder_h1': tf.Variable(tf.truncated_normal([num_hidden_2, num_hidden_1], stddev=0.1)),\n","        'decoder_h2': tf.Variable(tf.truncated_normal([num_hidden_1, num_input], stddev=0.1))\n","    }\n","\n","    biases = {\n","        'encoder_b1': tf.Variable(tf.constant(0.1, shape=[num_hidden_1])),\n","        'encoder_b2': tf.Variable(tf.constant(0.1, shape=[num_hidden_2])),\n","        # 'encoder_b3': tf.Variable(tf.constant(0.1, shape=[num_hidden_3])),\n","        # 'encoder_b4': tf.Variable(tf.constant(0.1, shape=[num_hidden_4])),\n","        # 'decoder_b1': tf.Variable(tf.constant(0.1, shape=[num_hidden_3])),\n","        # 'decoder_b2': tf.Variable(tf.constant(0.1, shape=[num_hidden_2])),\n","        'decoder_b1': tf.Variable(tf.constant(0.1, shape=[num_hidden_1])),\n","        'decoder_b2': tf.Variable(tf.constant(0.1, shape=[num_input]))\n","    }\n","\n","\n","    # Building the encoder\n","    def encoder(x):\n","        # Encoder Hidden layer with sigmoid activation #1\n","        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n","                                      biases['encoder_b1']))\n","        # Encoder Hidden layer with sigmoid activation #2\n","        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n","                                      biases['encoder_b2']))\n","        # layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['encoder_h3']),\n","        #                                biases['encoder_b3']))\n","        # layer_4 = tf.nn.sigmoid(tf.add(tf.matmul(layer_3, weights['encoder_h4']),\n","        #                                biases['encoder_b4']))\n","\n","        return layer_2\n","\n","\n","    # Building the decoder\n","    def decoder(x):\n","        # Decoder Hidden layer with sigmoid activation #1\n","        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n","                                      biases['decoder_b1']))\n","        # Decoder Hidden layer with sigmoid activation #2\n","        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n","                                      biases['decoder_b2']))\n","\n","        # layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['decoder_h3']),\n","        #                                biases['decoder_b3']))\n","        # layer_4 = tf.nn.sigmoid(tf.add(tf.matmul(layer_3, weights['decoder_h4']),\n","        #                                biases['decoder_b4']))\n","        return layer_2\n","\n","    # Construct model\n","    encoder_op = encoder(x)\n","    decoder_op = decoder(encoder_op)\n","\n","    # Prediction\n","    y_pred = decoder_op\n","    # Targets (Labels) are the input data.\n","    y_true = x\n","\n","    # ====================Neural decision forest=================================#\n","    num_out = 96\n","    num_output = 128\n","    N_nodes = 256\n","\n","    W_hidden = weight_variable([num_out, num_output])\n","    b_hidden = weight_variable([num_output])\n","    tree_input = tf.nn.relu(tf.matmul(encoder_op, W_hidden) + b_hidden)\n","    tree_input = tf.nn.dropout(tree_input, p_keep_conv)\n","\n","    w_t_ensemble = []\n","    w_d_ensemble = []\n","    w_l_ensemble = []\n","    for i in range(N_TREE):\n","        w_t_ensemble.append(weight_variable([num_output, N_nodes])) \n","        w_d_ensemble.append(init_prob_weights([N_nodes, N_LEAF], -1, 1)) \n","        w_l_ensemble.append(\n","            init_prob_weights([N_LEAF, N_LABEL], -2, 2))  \n","\n","    decision_p_e, leaf_p_e = model(tree_input, w_t_ensemble, w_d_ensemble, w_l_ensemble, p_keep_hidden)\n","    mu_e = neural_random_forest(decision_p_e, leaf_p_e, DEPTH, N_LEAF, N_BATCH)\n","    py_x = probability_y_x(mu_e, leaf_p_e) \n","\n","    # loss function\n","    cost = tf.add(tf.reduce_mean(-tf.multiply(tf.log(py_x), y)),tf.reduce_mean(tf.pow(y_true - y_pred, 2)))\n","    train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n","    predict = (tf.argmax(py_x, 1), py_x)\n","\n","    # =============== training ========================== #\n","    sess = tf.Session()\n","    sess.run(tf.global_variables_initializer())\n","\n","    largest_auc = 0.0\n","    counter = 0\n","    for i in range(num_epoch):\n","        # One epoch\n","        for start, end in zip(range(0, len(trX), N_BATCH), range(N_BATCH, len(trX), N_BATCH)):\n","            sess.run(train_step, feed_dict={x: trX[start:end], y: trY[start:end],\n","                                            p_keep_conv: 0.8, p_keep_hidden: 0.5})\n","        results = []\n","        prob = []\n","        true = []\n","        for start, end in zip(range(0, len(teX), N_BATCH), range(N_BATCH, len(teX), N_BATCH)):\n","            predicted = sess.run(predict, feed_dict={x: teX[start:end], p_keep_conv:1.0, p_keep_hidden: 1.0})\n","            results.extend(np.argmax(teY[start:end], axis=1) == predicted[0])\n","\n","            prob.extend(predicted[1][:,1])\n","            true.extend(np.argmax(teY[start:end], axis=1))\n","\n","        fpr, tpr, thresholds = metrics.roc_curve(true, prob, pos_label=1)\n","        auc_score = metrics.auc(fpr, tpr)\n","        print('Epoch: %d, Test Accuracy: %f, AUC: %f' % (i + 1, np.mean(results), auc_score))\n","\n","        if (auc_score > largest_auc):\n","            largest_auc = auc_score\n","        else:\n","            counter += 1\n","\n","        if (counter >= early_stop):\n","            print('Early stopping...AUC has not improved for %d rounds' % (counter))\n","            print('Best AUC: %f', (largest_auc))\n","            break\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzFvZTRJxKjB"},"source":["### Autoencoder ReLU Modification"]},{"cell_type":"code","metadata":{"id":"f9NB093BxBi9"},"source":["def run_model_relu(N_feature, trX, teX, trY, teY, num_epoch, early_stop):\n","\n","\n","    # Input X, output Y\n","    x = tf.placeholder(\"float\", [N_BATCH, N_feature])\n","    y = tf.placeholder(\"float\", [N_BATCH, N_LABEL])\n","\n","    p_keep_conv = tf.placeholder(\"float\")\n","    p_keep_hidden = tf.placeholder(\"float\")\n","\n","    # ====================Autoencoder========================================#\n","    num_hidden_1 = 64  \n","    num_hidden_2 = 96  \n","    # num_hidden_3 = 64\n","    # num_hidden_4 = 96\n","    num_input = N_feature  \n","\n","    weights = {\n","        'encoder_h1': tf.Variable(tf.truncated_normal([num_input, num_hidden_1], stddev=0.1)),\n","        'encoder_h2': tf.Variable(tf.truncated_normal([num_hidden_1, num_hidden_2], stddev=0.1)),\n","        # 'encoder_h3': tf.Variable(tf.truncated_normal([num_hidden_2, num_hidden_3], stddev=0.1)),\n","        # 'encoder_h4': tf.Variable(tf.truncated_normal([num_hidden_3, num_hidden_4], stddev=0.1)),\n","        # 'decoder_h1': tf.Variable(tf.truncated_normal([num_hidden_4, num_hidden_3], stddev=0.1)),\n","        # 'decoder_h2': tf.Variable(tf.truncated_normal([num_hidden_3, num_hidden_2], stddev=0.1)),\n","        'decoder_h1': tf.Variable(tf.truncated_normal([num_hidden_2, num_hidden_1], stddev=0.1)),\n","        'decoder_h2': tf.Variable(tf.truncated_normal([num_hidden_1, num_input], stddev=0.1))\n","    }\n","\n","    biases = {\n","        'encoder_b1': tf.Variable(tf.constant(0.1, shape=[num_hidden_1])),\n","        'encoder_b2': tf.Variable(tf.constant(0.1, shape=[num_hidden_2])),\n","        # 'encoder_b3': tf.Variable(tf.constant(0.1, shape=[num_hidden_3])),\n","        # 'encoder_b4': tf.Variable(tf.constant(0.1, shape=[num_hidden_4])),\n","        # 'decoder_b1': tf.Variable(tf.constant(0.1, shape=[num_hidden_3])),\n","        # 'decoder_b2': tf.Variable(tf.constant(0.1, shape=[num_hidden_2])),\n","        'decoder_b1': tf.Variable(tf.constant(0.1, shape=[num_hidden_1])),\n","        'decoder_b2': tf.Variable(tf.constant(0.1, shape=[num_input]))\n","    }\n","\n","\n","    # Building the encoder\n","    def encoder(x):\n","        # Encoder Hidden layer with sigmoid activation #1\n","        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['encoder_h1']),\n","                                      biases['encoder_b1']))\n","        # Encoder Hidden layer with sigmoid activation #2\n","        layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n","                                      biases['encoder_b2']))\n","        # layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['encoder_h3']),\n","        #                                biases['encoder_b3']))\n","        # layer_4 = tf.nn.sigmoid(tf.add(tf.matmul(layer_3, weights['encoder_h4']),\n","        #                                biases['encoder_b4']))\n","\n","        return layer_2\n","\n","\n","    # Building the decoder\n","    def decoder(x):\n","        # Decoder Hidden layer with sigmoid activation #1\n","        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['decoder_h1']),\n","                                      biases['decoder_b1']))\n","        # Decoder Hidden layer with sigmoid activation #2\n","        layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n","                                      biases['decoder_b2']))\n","\n","        # layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['decoder_h3']),\n","        #                                biases['decoder_b3']))\n","        # layer_4 = tf.nn.sigmoid(tf.add(tf.matmul(layer_3, weights['decoder_h4']),\n","        #                                biases['decoder_b4']))\n","        return layer_2\n","\n","    # Construct model\n","    encoder_op = encoder(x)\n","    decoder_op = decoder(encoder_op)\n","\n","    # Prediction\n","    y_pred = decoder_op\n","    # Targets (Labels) are the input data.\n","    y_true = x\n","\n","    # ====================Neural decision forest=================================#\n","    num_out = 96\n","    num_output = 128\n","    N_nodes = 256\n","\n","    W_hidden = weight_variable([num_out, num_output])\n","    b_hidden = weight_variable([num_output])\n","    tree_input = tf.nn.relu(tf.matmul(encoder_op, W_hidden) + b_hidden)\n","    tree_input = tf.nn.dropout(tree_input, p_keep_conv)\n","\n","    w_t_ensemble = []\n","    w_d_ensemble = []\n","    w_l_ensemble = []\n","    for i in range(N_TREE):\n","        w_t_ensemble.append(weight_variable([num_output, N_nodes])) \n","        w_d_ensemble.append(init_prob_weights([N_nodes, N_LEAF], -1, 1)) \n","        w_l_ensemble.append(\n","            init_prob_weights([N_LEAF, N_LABEL], -2, 2))  \n","\n","    decision_p_e, leaf_p_e = model(tree_input, w_t_ensemble, w_d_ensemble, w_l_ensemble, p_keep_hidden)\n","    mu_e = neural_random_forest(decision_p_e, leaf_p_e, DEPTH, N_LEAF, N_BATCH)\n","    py_x = probability_y_x(mu_e, leaf_p_e) \n","\n","    # loss function\n","    cost = tf.add(tf.reduce_mean(-tf.multiply(tf.log(py_x), y)),tf.reduce_mean(tf.pow(y_true - y_pred, 2)))\n","    train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n","    predict = (tf.argmax(py_x, 1), py_x)\n","\n","    # =============== training ========================== #\n","    sess = tf.Session()\n","    sess.run(tf.global_variables_initializer())\n","\n","    largest_auc = 0.0\n","    counter = 0\n","    for i in range(num_epoch):\n","        # One epoch\n","        for start, end in zip(range(0, len(trX), N_BATCH), range(N_BATCH, len(trX), N_BATCH)):\n","            sess.run(train_step, feed_dict={x: trX[start:end], y: trY[start:end],\n","                                            p_keep_conv: 0.8, p_keep_hidden: 0.5})\n","        results = []\n","        prob = []\n","        true = []\n","        for start, end in zip(range(0, len(teX), N_BATCH), range(N_BATCH, len(teX), N_BATCH)):\n","            predicted = sess.run(predict, feed_dict={x: teX[start:end], p_keep_conv:1.0, p_keep_hidden: 1.0})\n","            results.extend(np.argmax(teY[start:end], axis=1) == predicted[0])\n","\n","            prob.extend(predicted[1][:,1])\n","            true.extend(np.argmax(teY[start:end], axis=1))\n","\n","        fpr, tpr, thresholds = metrics.roc_curve(true, prob, pos_label=1)\n","        auc_score = metrics.auc(fpr, tpr)\n","        print('Epoch: %d, Test Accuracy: %f, AUC: %f' % (i + 1, np.mean(results), auc_score))\n","\n","        if (auc_score > largest_auc):\n","            largest_auc = auc_score\n","        else:\n","            counter += 1\n","\n","        if (counter >= early_stop):\n","            print('Early stopping...AUC has not improved for %d rounds' % (counter))\n","            print('Best AUC: %f', (largest_auc))\n","            break\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nS3U05y2X-Rk"},"source":["# Fit Models"]},{"cell_type":"markdown","metadata":{"id":"6FPGFfEkxZdM"},"source":["## Amazon Opinion Fraud"]},{"cell_type":"markdown","metadata":{"id":"HReT0mTsxh0f"},"source":["### Autoencoder + Neural DF"]},{"cell_type":"markdown","metadata":{"id":"2PQDeJZtxUXV"},"source":["#### Original (sigmoid)"]},{"cell_type":"code","metadata":{"id":"k2X-oOT4WUjw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636463115439,"user_tz":-480,"elapsed":66110,"user":{"displayName":"Badly taught","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11394682269253795404"}},"outputId":"57ca14a8-4f3c-4c86-ad9f-aafb5f5cef23"},"source":["start = datetime.datetime.now()\n","run_model(len(x_train_am[0]), x_train_am, x_val_am, y_train_am_rl, y_val_am_rl, 1000, 20)\n","time_taken = datetime.datetime.now() - start\n","print(time_taken)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Test Accuracy: 0.583462, AUC: 0.545567\n","Epoch: 2, Test Accuracy: 0.583462, AUC: 0.650805\n","Epoch: 3, Test Accuracy: 0.630385, AUC: 0.710681\n","Epoch: 4, Test Accuracy: 0.708077, AUC: 0.742312\n","Epoch: 5, Test Accuracy: 0.703077, AUC: 0.753090\n","Epoch: 6, Test Accuracy: 0.705000, AUC: 0.758091\n","Epoch: 7, Test Accuracy: 0.720000, AUC: 0.763273\n","Epoch: 8, Test Accuracy: 0.718462, AUC: 0.767938\n","Epoch: 9, Test Accuracy: 0.727692, AUC: 0.772420\n","Epoch: 10, Test Accuracy: 0.731154, AUC: 0.775746\n","Epoch: 11, Test Accuracy: 0.733077, AUC: 0.777267\n","Epoch: 12, Test Accuracy: 0.738077, AUC: 0.779760\n","Epoch: 13, Test Accuracy: 0.741923, AUC: 0.777944\n","Epoch: 14, Test Accuracy: 0.748077, AUC: 0.782222\n","Epoch: 15, Test Accuracy: 0.758077, AUC: 0.790767\n","Epoch: 16, Test Accuracy: 0.748846, AUC: 0.795866\n","Epoch: 17, Test Accuracy: 0.758462, AUC: 0.799020\n","Epoch: 18, Test Accuracy: 0.753846, AUC: 0.804213\n","Epoch: 19, Test Accuracy: 0.758846, AUC: 0.808781\n","Epoch: 20, Test Accuracy: 0.744615, AUC: 0.807920\n","Epoch: 21, Test Accuracy: 0.768846, AUC: 0.818203\n","Epoch: 22, Test Accuracy: 0.765000, AUC: 0.821992\n","Epoch: 23, Test Accuracy: 0.757308, AUC: 0.824640\n","Epoch: 24, Test Accuracy: 0.768077, AUC: 0.833431\n","Epoch: 25, Test Accuracy: 0.773077, AUC: 0.837302\n","Epoch: 26, Test Accuracy: 0.776154, AUC: 0.844287\n","Epoch: 27, Test Accuracy: 0.800769, AUC: 0.849806\n","Epoch: 28, Test Accuracy: 0.799231, AUC: 0.854786\n","Epoch: 29, Test Accuracy: 0.789615, AUC: 0.857584\n","Epoch: 30, Test Accuracy: 0.815000, AUC: 0.867577\n","Epoch: 31, Test Accuracy: 0.807308, AUC: 0.867454\n","Epoch: 32, Test Accuracy: 0.818077, AUC: 0.870070\n","Epoch: 33, Test Accuracy: 0.829615, AUC: 0.879965\n","Epoch: 34, Test Accuracy: 0.821923, AUC: 0.882542\n","Epoch: 35, Test Accuracy: 0.838462, AUC: 0.887002\n","Epoch: 36, Test Accuracy: 0.836923, AUC: 0.893896\n","Epoch: 37, Test Accuracy: 0.842692, AUC: 0.895271\n","Epoch: 38, Test Accuracy: 0.854615, AUC: 0.901958\n","Epoch: 39, Test Accuracy: 0.855385, AUC: 0.903978\n","Epoch: 40, Test Accuracy: 0.855385, AUC: 0.911423\n","Epoch: 41, Test Accuracy: 0.836154, AUC: 0.910603\n","Epoch: 42, Test Accuracy: 0.853077, AUC: 0.919260\n","Epoch: 43, Test Accuracy: 0.835769, AUC: 0.913339\n","Epoch: 44, Test Accuracy: 0.855769, AUC: 0.925285\n","Epoch: 45, Test Accuracy: 0.851923, AUC: 0.924466\n","Epoch: 46, Test Accuracy: 0.857692, AUC: 0.932148\n","Epoch: 47, Test Accuracy: 0.839615, AUC: 0.906080\n","Epoch: 48, Test Accuracy: 0.850000, AUC: 0.924448\n","Epoch: 49, Test Accuracy: 0.840385, AUC: 0.914860\n","Epoch: 50, Test Accuracy: 0.863462, AUC: 0.932368\n","Epoch: 51, Test Accuracy: 0.875769, AUC: 0.942501\n","Epoch: 52, Test Accuracy: 0.876923, AUC: 0.942589\n","Epoch: 53, Test Accuracy: 0.880385, AUC: 0.948117\n","Epoch: 54, Test Accuracy: 0.887308, AUC: 0.948264\n","Epoch: 55, Test Accuracy: 0.894231, AUC: 0.949157\n","Epoch: 56, Test Accuracy: 0.886923, AUC: 0.953293\n","Epoch: 57, Test Accuracy: 0.870000, AUC: 0.934536\n","Epoch: 58, Test Accuracy: 0.891538, AUC: 0.954057\n","Epoch: 59, Test Accuracy: 0.893462, AUC: 0.952232\n","Epoch: 60, Test Accuracy: 0.885385, AUC: 0.953452\n","Epoch: 61, Test Accuracy: 0.908462, AUC: 0.959589\n","Epoch: 62, Test Accuracy: 0.905385, AUC: 0.959580\n","Epoch: 63, Test Accuracy: 0.910385, AUC: 0.960426\n","Epoch: 64, Test Accuracy: 0.896538, AUC: 0.952191\n","Epoch: 65, Test Accuracy: 0.905000, AUC: 0.958600\n","Epoch: 66, Test Accuracy: 0.880385, AUC: 0.956381\n","Epoch: 67, Test Accuracy: 0.921538, AUC: 0.964906\n","Epoch: 68, Test Accuracy: 0.913462, AUC: 0.965865\n","Epoch: 69, Test Accuracy: 0.930385, AUC: 0.969763\n","Epoch: 70, Test Accuracy: 0.914231, AUC: 0.966646\n","Epoch: 71, Test Accuracy: 0.913462, AUC: 0.964875\n","Epoch: 72, Test Accuracy: 0.918462, AUC: 0.964177\n","Epoch: 73, Test Accuracy: 0.908846, AUC: 0.961372\n","Early stopping...AUC has not improved for 20 rounds\n","Best AUC: %f 0.9697634260163819\n","0:01:05.617651\n"]}]},{"cell_type":"markdown","metadata":{"id":"b-6TlOShxSTp"},"source":["#### ReLU version"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ouKVpfWixR01","executionInfo":{"status":"ok","timestamp":1636463162564,"user_tz":-480,"elapsed":42646,"user":{"displayName":"Badly taught","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11394682269253795404"}},"outputId":"6e338f9e-ae0e-4704-a9fd-f32acdd3bd90"},"source":["start = datetime.datetime.now()\n","run_model_relu(len(x_train_am[0]), x_train_am, x_val_am, y_train_am_rl, y_val_am_rl, 1000, 20)\n","time_taken = datetime.datetime.now() - start\n","print(time_taken)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Test Accuracy: 0.623077, AUC: 0.693669\n","Epoch: 2, Test Accuracy: 0.725000, AUC: 0.774613\n","Epoch: 3, Test Accuracy: 0.754615, AUC: 0.815221\n","Epoch: 4, Test Accuracy: 0.780000, AUC: 0.844526\n","Epoch: 5, Test Accuracy: 0.823846, AUC: 0.877797\n","Epoch: 6, Test Accuracy: 0.833077, AUC: 0.894082\n","Epoch: 7, Test Accuracy: 0.848846, AUC: 0.915288\n","Epoch: 8, Test Accuracy: 0.854615, AUC: 0.928205\n","Epoch: 9, Test Accuracy: 0.874231, AUC: 0.946706\n","Epoch: 10, Test Accuracy: 0.888077, AUC: 0.949047\n","Epoch: 11, Test Accuracy: 0.880385, AUC: 0.956342\n","Epoch: 12, Test Accuracy: 0.896538, AUC: 0.962600\n","Epoch: 13, Test Accuracy: 0.909231, AUC: 0.967635\n","Epoch: 14, Test Accuracy: 0.903846, AUC: 0.961963\n","Epoch: 15, Test Accuracy: 0.922692, AUC: 0.973405\n","Epoch: 16, Test Accuracy: 0.932692, AUC: 0.975755\n","Epoch: 17, Test Accuracy: 0.916923, AUC: 0.970754\n","Epoch: 18, Test Accuracy: 0.932692, AUC: 0.975758\n","Epoch: 19, Test Accuracy: 0.941154, AUC: 0.980916\n","Epoch: 20, Test Accuracy: 0.935385, AUC: 0.975057\n","Epoch: 21, Test Accuracy: 0.938462, AUC: 0.976807\n","Epoch: 22, Test Accuracy: 0.937692, AUC: 0.978798\n","Epoch: 23, Test Accuracy: 0.940769, AUC: 0.981496\n","Epoch: 24, Test Accuracy: 0.946538, AUC: 0.979988\n","Epoch: 25, Test Accuracy: 0.945385, AUC: 0.980675\n","Epoch: 26, Test Accuracy: 0.945000, AUC: 0.981401\n","Epoch: 27, Test Accuracy: 0.946538, AUC: 0.982125\n","Epoch: 28, Test Accuracy: 0.951923, AUC: 0.983783\n","Epoch: 29, Test Accuracy: 0.954615, AUC: 0.983516\n","Epoch: 30, Test Accuracy: 0.946923, AUC: 0.980557\n","Epoch: 31, Test Accuracy: 0.956154, AUC: 0.984604\n","Epoch: 32, Test Accuracy: 0.949231, AUC: 0.982802\n","Epoch: 33, Test Accuracy: 0.957308, AUC: 0.985623\n","Epoch: 34, Test Accuracy: 0.956538, AUC: 0.984827\n","Epoch: 35, Test Accuracy: 0.953846, AUC: 0.983113\n","Epoch: 36, Test Accuracy: 0.954615, AUC: 0.984370\n","Epoch: 37, Test Accuracy: 0.955385, AUC: 0.983353\n","Epoch: 38, Test Accuracy: 0.962308, AUC: 0.987485\n","Epoch: 39, Test Accuracy: 0.960769, AUC: 0.986679\n","Epoch: 40, Test Accuracy: 0.944615, AUC: 0.979173\n","Epoch: 41, Test Accuracy: 0.951538, AUC: 0.983972\n","Epoch: 42, Test Accuracy: 0.959231, AUC: 0.986934\n","Epoch: 43, Test Accuracy: 0.958846, AUC: 0.985702\n","Early stopping...AUC has not improved for 20 rounds\n","Best AUC: %f 0.9874853233072273\n","0:00:42.100819\n"]}]},{"cell_type":"markdown","metadata":{"id":"2EwkWFm9EYUl"},"source":["### XGBoost"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6gE4L0tEaGh","executionInfo":{"status":"ok","timestamp":1636458285685,"user_tz":-480,"elapsed":5797,"user":{"displayName":"Badly taught","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11394682269253795404"}},"outputId":"747fee24-47bd-4d46-a01d-1e7c02cd753e"},"source":["start = datetime.datetime.now()\n","\n","xgbmodel_am = XGBClassifier(objective='binary:logistic',  n_estimators = 1000, booster = 'gbtree', eval_metric = ['auc'])\n","xgbmodel_am.fit(x_train_am, y_train_am, eval_set = [(x_val_am, y_val_am)], early_stopping_rounds = 10)\n","time_taken = datetime.datetime.now() - start\n","print(time_taken)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0]\tvalidation_0-auc:0.771731\n","Will train until validation_0-auc hasn't improved in 10 rounds.\n","[1]\tvalidation_0-auc:0.807045\n","[2]\tvalidation_0-auc:0.813855\n","[3]\tvalidation_0-auc:0.822107\n","[4]\tvalidation_0-auc:0.836414\n","[5]\tvalidation_0-auc:0.836386\n","[6]\tvalidation_0-auc:0.849798\n","[7]\tvalidation_0-auc:0.850475\n","[8]\tvalidation_0-auc:0.852863\n","[9]\tvalidation_0-auc:0.859037\n","[10]\tvalidation_0-auc:0.865218\n","[11]\tvalidation_0-auc:0.868144\n","[12]\tvalidation_0-auc:0.875424\n","[13]\tvalidation_0-auc:0.879622\n","[14]\tvalidation_0-auc:0.88507\n","[15]\tvalidation_0-auc:0.887647\n","[16]\tvalidation_0-auc:0.890419\n","[17]\tvalidation_0-auc:0.893082\n","[18]\tvalidation_0-auc:0.89435\n","[19]\tvalidation_0-auc:0.897332\n","[20]\tvalidation_0-auc:0.900616\n","[21]\tvalidation_0-auc:0.901814\n","[22]\tvalidation_0-auc:0.904788\n","[23]\tvalidation_0-auc:0.906233\n","[24]\tvalidation_0-auc:0.907148\n","[25]\tvalidation_0-auc:0.908366\n","[26]\tvalidation_0-auc:0.910039\n","[27]\tvalidation_0-auc:0.912891\n","[28]\tvalidation_0-auc:0.915357\n","[29]\tvalidation_0-auc:0.917449\n","[30]\tvalidation_0-auc:0.919596\n","[31]\tvalidation_0-auc:0.922036\n","[32]\tvalidation_0-auc:0.923628\n","[33]\tvalidation_0-auc:0.925335\n","[34]\tvalidation_0-auc:0.927104\n","[35]\tvalidation_0-auc:0.928761\n","[36]\tvalidation_0-auc:0.929544\n","[37]\tvalidation_0-auc:0.931761\n","[38]\tvalidation_0-auc:0.933583\n","[39]\tvalidation_0-auc:0.934589\n","[40]\tvalidation_0-auc:0.936174\n","[41]\tvalidation_0-auc:0.938691\n","[42]\tvalidation_0-auc:0.939669\n","[43]\tvalidation_0-auc:0.942019\n","[44]\tvalidation_0-auc:0.94275\n","[45]\tvalidation_0-auc:0.943961\n","[46]\tvalidation_0-auc:0.945089\n","[47]\tvalidation_0-auc:0.946597\n","[48]\tvalidation_0-auc:0.947811\n","[49]\tvalidation_0-auc:0.948721\n","[50]\tvalidation_0-auc:0.949587\n","[51]\tvalidation_0-auc:0.950541\n","[52]\tvalidation_0-auc:0.95196\n","[53]\tvalidation_0-auc:0.952394\n","[54]\tvalidation_0-auc:0.952972\n","[55]\tvalidation_0-auc:0.953537\n","[56]\tvalidation_0-auc:0.95403\n","[57]\tvalidation_0-auc:0.955527\n","[58]\tvalidation_0-auc:0.956542\n","[59]\tvalidation_0-auc:0.957705\n","[60]\tvalidation_0-auc:0.959464\n","[61]\tvalidation_0-auc:0.959977\n","[62]\tvalidation_0-auc:0.960278\n","[63]\tvalidation_0-auc:0.960752\n","[64]\tvalidation_0-auc:0.960614\n","[65]\tvalidation_0-auc:0.960975\n","[66]\tvalidation_0-auc:0.962154\n","[67]\tvalidation_0-auc:0.962447\n","[68]\tvalidation_0-auc:0.96333\n","[69]\tvalidation_0-auc:0.964137\n","[70]\tvalidation_0-auc:0.965315\n","[71]\tvalidation_0-auc:0.9659\n","[72]\tvalidation_0-auc:0.966421\n","[73]\tvalidation_0-auc:0.967354\n","[74]\tvalidation_0-auc:0.968295\n","[75]\tvalidation_0-auc:0.968868\n","[76]\tvalidation_0-auc:0.96943\n","[77]\tvalidation_0-auc:0.970112\n","[78]\tvalidation_0-auc:0.970358\n","[79]\tvalidation_0-auc:0.97071\n","[80]\tvalidation_0-auc:0.970736\n","[81]\tvalidation_0-auc:0.971297\n","[82]\tvalidation_0-auc:0.971241\n","[83]\tvalidation_0-auc:0.971832\n","[84]\tvalidation_0-auc:0.972605\n","[85]\tvalidation_0-auc:0.973592\n","[86]\tvalidation_0-auc:0.974016\n","[87]\tvalidation_0-auc:0.974096\n","[88]\tvalidation_0-auc:0.974349\n","[89]\tvalidation_0-auc:0.974616\n","[90]\tvalidation_0-auc:0.974856\n","[91]\tvalidation_0-auc:0.975356\n","[92]\tvalidation_0-auc:0.975602\n","[93]\tvalidation_0-auc:0.975854\n","[94]\tvalidation_0-auc:0.975972\n","[95]\tvalidation_0-auc:0.975858\n","[96]\tvalidation_0-auc:0.976213\n","[97]\tvalidation_0-auc:0.976639\n","[98]\tvalidation_0-auc:0.976954\n","[99]\tvalidation_0-auc:0.977451\n","[100]\tvalidation_0-auc:0.97784\n","[101]\tvalidation_0-auc:0.977979\n","[102]\tvalidation_0-auc:0.978197\n","[103]\tvalidation_0-auc:0.978141\n","[104]\tvalidation_0-auc:0.978453\n","[105]\tvalidation_0-auc:0.978767\n","[106]\tvalidation_0-auc:0.979169\n","[107]\tvalidation_0-auc:0.979375\n","[108]\tvalidation_0-auc:0.98005\n","[109]\tvalidation_0-auc:0.980752\n","[110]\tvalidation_0-auc:0.980957\n","[111]\tvalidation_0-auc:0.981823\n","[112]\tvalidation_0-auc:0.981917\n","[113]\tvalidation_0-auc:0.982172\n","[114]\tvalidation_0-auc:0.982676\n","[115]\tvalidation_0-auc:0.983256\n","[116]\tvalidation_0-auc:0.983336\n","[117]\tvalidation_0-auc:0.983582\n","[118]\tvalidation_0-auc:0.983683\n","[119]\tvalidation_0-auc:0.983827\n","[120]\tvalidation_0-auc:0.983913\n","[121]\tvalidation_0-auc:0.984077\n","[122]\tvalidation_0-auc:0.984194\n","[123]\tvalidation_0-auc:0.984372\n","[124]\tvalidation_0-auc:0.98468\n","[125]\tvalidation_0-auc:0.984985\n","[126]\tvalidation_0-auc:0.985031\n","[127]\tvalidation_0-auc:0.985129\n","[128]\tvalidation_0-auc:0.985452\n","[129]\tvalidation_0-auc:0.985691\n","[130]\tvalidation_0-auc:0.986082\n","[131]\tvalidation_0-auc:0.986293\n","[132]\tvalidation_0-auc:0.986597\n","[133]\tvalidation_0-auc:0.987068\n","[134]\tvalidation_0-auc:0.987145\n","[135]\tvalidation_0-auc:0.987566\n","[136]\tvalidation_0-auc:0.987671\n","[137]\tvalidation_0-auc:0.987784\n","[138]\tvalidation_0-auc:0.988085\n","[139]\tvalidation_0-auc:0.988225\n","[140]\tvalidation_0-auc:0.988326\n","[141]\tvalidation_0-auc:0.988496\n","[142]\tvalidation_0-auc:0.988585\n","[143]\tvalidation_0-auc:0.988634\n","[144]\tvalidation_0-auc:0.988871\n","[145]\tvalidation_0-auc:0.988832\n","[146]\tvalidation_0-auc:0.988903\n","[147]\tvalidation_0-auc:0.9891\n","[148]\tvalidation_0-auc:0.989082\n","[149]\tvalidation_0-auc:0.989105\n","[150]\tvalidation_0-auc:0.989269\n","[151]\tvalidation_0-auc:0.989333\n","[152]\tvalidation_0-auc:0.989373\n","[153]\tvalidation_0-auc:0.989683\n","[154]\tvalidation_0-auc:0.990347\n","[155]\tvalidation_0-auc:0.990391\n","[156]\tvalidation_0-auc:0.990613\n","[157]\tvalidation_0-auc:0.990708\n","[158]\tvalidation_0-auc:0.990706\n","[159]\tvalidation_0-auc:0.990739\n","[160]\tvalidation_0-auc:0.990933\n","[161]\tvalidation_0-auc:0.991193\n","[162]\tvalidation_0-auc:0.991184\n","[163]\tvalidation_0-auc:0.991631\n","[164]\tvalidation_0-auc:0.991649\n","[165]\tvalidation_0-auc:0.991914\n","[166]\tvalidation_0-auc:0.992374\n","[167]\tvalidation_0-auc:0.992441\n","[168]\tvalidation_0-auc:0.992453\n","[169]\tvalidation_0-auc:0.992519\n","[170]\tvalidation_0-auc:0.992558\n","[171]\tvalidation_0-auc:0.992601\n","[172]\tvalidation_0-auc:0.992604\n","[173]\tvalidation_0-auc:0.9928\n","[174]\tvalidation_0-auc:0.992815\n","[175]\tvalidation_0-auc:0.992818\n","[176]\tvalidation_0-auc:0.992787\n","[177]\tvalidation_0-auc:0.993031\n","[178]\tvalidation_0-auc:0.993114\n","[179]\tvalidation_0-auc:0.993291\n","[180]\tvalidation_0-auc:0.993196\n","[181]\tvalidation_0-auc:0.993398\n","[182]\tvalidation_0-auc:0.993417\n","[183]\tvalidation_0-auc:0.993512\n","[184]\tvalidation_0-auc:0.993543\n","[185]\tvalidation_0-auc:0.993511\n","[186]\tvalidation_0-auc:0.993485\n","[187]\tvalidation_0-auc:0.993724\n","[188]\tvalidation_0-auc:0.993739\n","[189]\tvalidation_0-auc:0.993708\n","[190]\tvalidation_0-auc:0.993793\n","[191]\tvalidation_0-auc:0.993815\n","[192]\tvalidation_0-auc:0.993997\n","[193]\tvalidation_0-auc:0.994153\n","[194]\tvalidation_0-auc:0.994164\n","[195]\tvalidation_0-auc:0.994188\n","[196]\tvalidation_0-auc:0.99421\n","[197]\tvalidation_0-auc:0.994229\n","[198]\tvalidation_0-auc:0.994311\n","[199]\tvalidation_0-auc:0.994342\n","[200]\tvalidation_0-auc:0.99436\n","[201]\tvalidation_0-auc:0.994376\n","[202]\tvalidation_0-auc:0.994385\n","[203]\tvalidation_0-auc:0.99443\n","[204]\tvalidation_0-auc:0.994449\n","[205]\tvalidation_0-auc:0.994446\n","[206]\tvalidation_0-auc:0.994532\n","[207]\tvalidation_0-auc:0.994518\n","[208]\tvalidation_0-auc:0.994648\n","[209]\tvalidation_0-auc:0.99471\n","[210]\tvalidation_0-auc:0.9947\n","[211]\tvalidation_0-auc:0.994826\n","[212]\tvalidation_0-auc:0.99493\n","[213]\tvalidation_0-auc:0.994996\n","[214]\tvalidation_0-auc:0.995036\n","[215]\tvalidation_0-auc:0.995078\n","[216]\tvalidation_0-auc:0.995194\n","[217]\tvalidation_0-auc:0.99528\n","[218]\tvalidation_0-auc:0.995248\n","[219]\tvalidation_0-auc:0.99524\n","[220]\tvalidation_0-auc:0.995391\n","[221]\tvalidation_0-auc:0.995442\n","[222]\tvalidation_0-auc:0.995457\n","[223]\tvalidation_0-auc:0.9955\n","[224]\tvalidation_0-auc:0.995632\n","[225]\tvalidation_0-auc:0.995738\n","[226]\tvalidation_0-auc:0.995799\n","[227]\tvalidation_0-auc:0.995798\n","[228]\tvalidation_0-auc:0.995899\n","[229]\tvalidation_0-auc:0.995906\n","[230]\tvalidation_0-auc:0.995992\n","[231]\tvalidation_0-auc:0.995975\n","[232]\tvalidation_0-auc:0.996044\n","[233]\tvalidation_0-auc:0.996049\n","[234]\tvalidation_0-auc:0.996131\n","[235]\tvalidation_0-auc:0.996158\n","[236]\tvalidation_0-auc:0.996177\n","[237]\tvalidation_0-auc:0.996279\n","[238]\tvalidation_0-auc:0.996428\n","[239]\tvalidation_0-auc:0.996437\n","[240]\tvalidation_0-auc:0.996431\n","[241]\tvalidation_0-auc:0.996437\n","[242]\tvalidation_0-auc:0.996532\n","[243]\tvalidation_0-auc:0.996521\n","[244]\tvalidation_0-auc:0.9965\n","[245]\tvalidation_0-auc:0.996576\n","[246]\tvalidation_0-auc:0.996562\n","[247]\tvalidation_0-auc:0.996603\n","[248]\tvalidation_0-auc:0.996614\n","[249]\tvalidation_0-auc:0.996711\n","[250]\tvalidation_0-auc:0.996672\n","[251]\tvalidation_0-auc:0.996724\n","[252]\tvalidation_0-auc:0.996758\n","[253]\tvalidation_0-auc:0.996825\n","[254]\tvalidation_0-auc:0.996827\n","[255]\tvalidation_0-auc:0.996853\n","[256]\tvalidation_0-auc:0.996867\n","[257]\tvalidation_0-auc:0.996862\n","[258]\tvalidation_0-auc:0.996873\n","[259]\tvalidation_0-auc:0.996979\n","[260]\tvalidation_0-auc:0.996984\n","[261]\tvalidation_0-auc:0.99698\n","[262]\tvalidation_0-auc:0.996985\n","[263]\tvalidation_0-auc:0.997083\n","[264]\tvalidation_0-auc:0.997098\n","[265]\tvalidation_0-auc:0.997098\n","[266]\tvalidation_0-auc:0.997163\n","[267]\tvalidation_0-auc:0.997187\n","[268]\tvalidation_0-auc:0.997255\n","[269]\tvalidation_0-auc:0.997356\n","[270]\tvalidation_0-auc:0.997349\n","[271]\tvalidation_0-auc:0.997397\n","[272]\tvalidation_0-auc:0.997489\n","[273]\tvalidation_0-auc:0.997511\n","[274]\tvalidation_0-auc:0.997495\n","[275]\tvalidation_0-auc:0.997475\n","[276]\tvalidation_0-auc:0.997487\n","[277]\tvalidation_0-auc:0.997528\n","[278]\tvalidation_0-auc:0.997568\n","[279]\tvalidation_0-auc:0.997582\n","[280]\tvalidation_0-auc:0.997608\n","[281]\tvalidation_0-auc:0.997669\n","[282]\tvalidation_0-auc:0.997683\n","[283]\tvalidation_0-auc:0.997694\n","[284]\tvalidation_0-auc:0.997736\n","[285]\tvalidation_0-auc:0.997779\n","[286]\tvalidation_0-auc:0.997798\n","[287]\tvalidation_0-auc:0.997836\n","[288]\tvalidation_0-auc:0.997844\n","[289]\tvalidation_0-auc:0.997858\n","[290]\tvalidation_0-auc:0.997863\n","[291]\tvalidation_0-auc:0.997837\n","[292]\tvalidation_0-auc:0.997844\n","[293]\tvalidation_0-auc:0.997873\n","[294]\tvalidation_0-auc:0.997894\n","[295]\tvalidation_0-auc:0.997904\n","[296]\tvalidation_0-auc:0.997886\n","[297]\tvalidation_0-auc:0.997948\n","[298]\tvalidation_0-auc:0.997955\n","[299]\tvalidation_0-auc:0.997964\n","[300]\tvalidation_0-auc:0.997986\n","[301]\tvalidation_0-auc:0.997983\n","[302]\tvalidation_0-auc:0.997942\n","[303]\tvalidation_0-auc:0.997958\n","[304]\tvalidation_0-auc:0.997971\n","[305]\tvalidation_0-auc:0.997965\n","[306]\tvalidation_0-auc:0.997981\n","[307]\tvalidation_0-auc:0.99795\n","[308]\tvalidation_0-auc:0.997972\n","[309]\tvalidation_0-auc:0.997967\n","[310]\tvalidation_0-auc:0.998034\n","[311]\tvalidation_0-auc:0.998075\n","[312]\tvalidation_0-auc:0.998113\n","[313]\tvalidation_0-auc:0.998142\n","[314]\tvalidation_0-auc:0.998168\n","[315]\tvalidation_0-auc:0.99817\n","[316]\tvalidation_0-auc:0.998175\n","[317]\tvalidation_0-auc:0.998194\n","[318]\tvalidation_0-auc:0.998172\n","[319]\tvalidation_0-auc:0.998164\n","[320]\tvalidation_0-auc:0.998174\n","[321]\tvalidation_0-auc:0.998221\n","[322]\tvalidation_0-auc:0.998248\n","[323]\tvalidation_0-auc:0.99825\n","[324]\tvalidation_0-auc:0.998257\n","[325]\tvalidation_0-auc:0.99825\n","[326]\tvalidation_0-auc:0.998255\n","[327]\tvalidation_0-auc:0.998284\n","[328]\tvalidation_0-auc:0.998288\n","[329]\tvalidation_0-auc:0.998321\n","[330]\tvalidation_0-auc:0.998338\n","[331]\tvalidation_0-auc:0.998336\n","[332]\tvalidation_0-auc:0.998333\n","[333]\tvalidation_0-auc:0.998332\n","[334]\tvalidation_0-auc:0.998349\n","[335]\tvalidation_0-auc:0.998345\n","[336]\tvalidation_0-auc:0.998343\n","[337]\tvalidation_0-auc:0.998343\n","[338]\tvalidation_0-auc:0.998343\n","[339]\tvalidation_0-auc:0.998343\n","[340]\tvalidation_0-auc:0.998327\n","[341]\tvalidation_0-auc:0.998356\n","[342]\tvalidation_0-auc:0.99834\n","[343]\tvalidation_0-auc:0.998348\n","[344]\tvalidation_0-auc:0.998357\n","[345]\tvalidation_0-auc:0.99831\n","[346]\tvalidation_0-auc:0.998308\n","[347]\tvalidation_0-auc:0.998319\n","[348]\tvalidation_0-auc:0.99831\n","[349]\tvalidation_0-auc:0.998322\n","[350]\tvalidation_0-auc:0.998349\n","[351]\tvalidation_0-auc:0.998386\n","[352]\tvalidation_0-auc:0.998381\n","[353]\tvalidation_0-auc:0.998365\n","[354]\tvalidation_0-auc:0.998332\n","[355]\tvalidation_0-auc:0.99833\n","[356]\tvalidation_0-auc:0.99837\n","[357]\tvalidation_0-auc:0.998329\n","[358]\tvalidation_0-auc:0.998348\n","[359]\tvalidation_0-auc:0.998343\n","[360]\tvalidation_0-auc:0.998346\n","[361]\tvalidation_0-auc:0.998349\n","Stopping. Best iteration:\n","[351]\tvalidation_0-auc:0.998386\n","\n","0:00:05.412893\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZZGOHTJQbe_u"},"source":["## Ethereum Fraud"]},{"cell_type":"markdown","metadata":{"id":"m_IjZbghbul1"},"source":["### Autoencoder + Neural DF"]},{"cell_type":"markdown","metadata":{"id":"iz0Sp5WoU4DQ"},"source":["#### Original (sigmoid)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RacBLcwzbkXh","executionInfo":{"status":"ok","timestamp":1636463401785,"user_tz":-480,"elapsed":69331,"user":{"displayName":"Badly taught","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11394682269253795404"}},"outputId":"8d6ade22-3cc5-4102-af67-8d352477d59c"},"source":["start = datetime.datetime.now()\n","run_model(45, x_train_eth, x_val_eth, y_train_eth_rl, y_val_eth_rl, 1000, 20)\n","time_taken = datetime.datetime.now() - start\n","print(time_taken)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Test Accuracy: 0.779375, AUC: 0.714578\n","Epoch: 2, Test Accuracy: 0.779375, AUC: 0.756671\n","Epoch: 3, Test Accuracy: 0.779375, AUC: 0.778444\n","Epoch: 4, Test Accuracy: 0.779375, AUC: 0.736773\n","Epoch: 5, Test Accuracy: 0.779375, AUC: 0.744141\n","Epoch: 6, Test Accuracy: 0.779375, AUC: 0.769103\n","Epoch: 7, Test Accuracy: 0.779375, AUC: 0.781056\n","Epoch: 8, Test Accuracy: 0.779375, AUC: 0.790193\n","Epoch: 9, Test Accuracy: 0.779375, AUC: 0.805775\n","Epoch: 10, Test Accuracy: 0.804063, AUC: 0.820459\n","Epoch: 11, Test Accuracy: 0.807813, AUC: 0.832553\n","Epoch: 12, Test Accuracy: 0.770625, AUC: 0.839883\n","Epoch: 13, Test Accuracy: 0.803125, AUC: 0.844497\n","Epoch: 14, Test Accuracy: 0.808125, AUC: 0.846074\n","Epoch: 15, Test Accuracy: 0.791562, AUC: 0.849728\n","Epoch: 16, Test Accuracy: 0.809688, AUC: 0.852073\n","Epoch: 17, Test Accuracy: 0.772813, AUC: 0.855094\n","Epoch: 18, Test Accuracy: 0.808438, AUC: 0.856390\n","Epoch: 19, Test Accuracy: 0.790000, AUC: 0.861165\n","Epoch: 20, Test Accuracy: 0.783438, AUC: 0.864191\n","Epoch: 21, Test Accuracy: 0.771250, AUC: 0.865732\n","Epoch: 22, Test Accuracy: 0.783438, AUC: 0.869960\n","Epoch: 23, Test Accuracy: 0.800000, AUC: 0.875283\n","Epoch: 24, Test Accuracy: 0.758125, AUC: 0.872474\n","Epoch: 25, Test Accuracy: 0.791250, AUC: 0.875147\n","Epoch: 26, Test Accuracy: 0.778125, AUC: 0.875095\n","Epoch: 27, Test Accuracy: 0.779375, AUC: 0.874639\n","Epoch: 28, Test Accuracy: 0.731563, AUC: 0.881026\n","Epoch: 29, Test Accuracy: 0.747500, AUC: 0.875758\n","Epoch: 30, Test Accuracy: 0.800937, AUC: 0.883012\n","Epoch: 31, Test Accuracy: 0.798750, AUC: 0.884148\n","Epoch: 32, Test Accuracy: 0.789062, AUC: 0.878383\n","Epoch: 33, Test Accuracy: 0.814688, AUC: 0.884663\n","Epoch: 34, Test Accuracy: 0.758750, AUC: 0.884847\n","Epoch: 35, Test Accuracy: 0.776875, AUC: 0.882272\n","Epoch: 36, Test Accuracy: 0.802188, AUC: 0.885971\n","Epoch: 37, Test Accuracy: 0.784687, AUC: 0.887037\n","Epoch: 38, Test Accuracy: 0.798750, AUC: 0.888306\n","Epoch: 39, Test Accuracy: 0.798750, AUC: 0.890184\n","Epoch: 40, Test Accuracy: 0.806250, AUC: 0.892663\n","Epoch: 41, Test Accuracy: 0.811562, AUC: 0.891967\n","Epoch: 42, Test Accuracy: 0.803750, AUC: 0.893834\n","Epoch: 43, Test Accuracy: 0.799687, AUC: 0.890069\n","Epoch: 44, Test Accuracy: 0.805625, AUC: 0.895416\n","Epoch: 45, Test Accuracy: 0.808125, AUC: 0.897329\n","Epoch: 46, Test Accuracy: 0.799687, AUC: 0.897487\n","Epoch: 47, Test Accuracy: 0.795000, AUC: 0.896104\n","Epoch: 48, Test Accuracy: 0.809063, AUC: 0.899501\n","Epoch: 49, Test Accuracy: 0.801562, AUC: 0.898561\n","Epoch: 50, Test Accuracy: 0.802188, AUC: 0.900242\n","Epoch: 51, Test Accuracy: 0.800000, AUC: 0.901235\n","Epoch: 52, Test Accuracy: 0.813750, AUC: 0.904131\n","Epoch: 53, Test Accuracy: 0.811250, AUC: 0.905488\n","Epoch: 54, Test Accuracy: 0.806562, AUC: 0.902474\n","Epoch: 55, Test Accuracy: 0.807500, AUC: 0.903165\n","Epoch: 56, Test Accuracy: 0.812500, AUC: 0.900183\n","Epoch: 57, Test Accuracy: 0.812187, AUC: 0.905296\n","Epoch: 58, Test Accuracy: 0.822812, AUC: 0.908257\n","Epoch: 59, Test Accuracy: 0.797813, AUC: 0.902009\n","Epoch: 60, Test Accuracy: 0.805312, AUC: 0.906938\n","Early stopping...AUC has not improved for 20 rounds\n","Best AUC: %f 0.9082571542807554\n","0:01:08.935005\n"]}]},{"cell_type":"markdown","metadata":{"id":"79cf5Gbpxd7X"},"source":["#### ReLU version"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DmJrVGZlxe9H","executionInfo":{"status":"ok","timestamp":1636463461314,"user_tz":-480,"elapsed":44630,"user":{"displayName":"Badly taught","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11394682269253795404"}},"outputId":"034271dd-91a5-48d7-bb4f-cce648c7b82a"},"source":["start = datetime.datetime.now()\n","run_model_relu(45, x_train_eth, x_val_eth, y_train_eth_rl, y_val_eth_rl, 1000, 20)\n","time_taken = datetime.datetime.now() - start\n","print(time_taken)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Test Accuracy: 0.779375, AUC: 0.804694\n","Epoch: 2, Test Accuracy: 0.815000, AUC: 0.867953\n","Epoch: 3, Test Accuracy: 0.815312, AUC: 0.891781\n","Epoch: 4, Test Accuracy: 0.820625, AUC: 0.902536\n","Epoch: 5, Test Accuracy: 0.817500, AUC: 0.907120\n","Epoch: 6, Test Accuracy: 0.834688, AUC: 0.915299\n","Epoch: 7, Test Accuracy: 0.847500, AUC: 0.919788\n","Epoch: 8, Test Accuracy: 0.848750, AUC: 0.922853\n","Epoch: 9, Test Accuracy: 0.849063, AUC: 0.924356\n","Epoch: 10, Test Accuracy: 0.852187, AUC: 0.927600\n","Epoch: 11, Test Accuracy: 0.854375, AUC: 0.927807\n","Epoch: 12, Test Accuracy: 0.885000, AUC: 0.932084\n","Epoch: 13, Test Accuracy: 0.858437, AUC: 0.928463\n","Epoch: 14, Test Accuracy: 0.883750, AUC: 0.934856\n","Epoch: 15, Test Accuracy: 0.880313, AUC: 0.932314\n","Epoch: 16, Test Accuracy: 0.857812, AUC: 0.934710\n","Epoch: 17, Test Accuracy: 0.850938, AUC: 0.931651\n","Epoch: 18, Test Accuracy: 0.887500, AUC: 0.936734\n","Epoch: 19, Test Accuracy: 0.893437, AUC: 0.936907\n","Epoch: 20, Test Accuracy: 0.896250, AUC: 0.942115\n","Epoch: 21, Test Accuracy: 0.845625, AUC: 0.937101\n","Epoch: 22, Test Accuracy: 0.895938, AUC: 0.941012\n","Epoch: 23, Test Accuracy: 0.891563, AUC: 0.939724\n","Epoch: 24, Test Accuracy: 0.894375, AUC: 0.940811\n","Epoch: 25, Test Accuracy: 0.895000, AUC: 0.941626\n","Epoch: 26, Test Accuracy: 0.895312, AUC: 0.940577\n","Epoch: 27, Test Accuracy: 0.901250, AUC: 0.944786\n","Epoch: 28, Test Accuracy: 0.896875, AUC: 0.942413\n","Epoch: 29, Test Accuracy: 0.875000, AUC: 0.941621\n","Epoch: 30, Test Accuracy: 0.842187, AUC: 0.941130\n","Epoch: 31, Test Accuracy: 0.869375, AUC: 0.939537\n","Epoch: 32, Test Accuracy: 0.892188, AUC: 0.942977\n","Epoch: 33, Test Accuracy: 0.864062, AUC: 0.941401\n","Epoch: 34, Test Accuracy: 0.829375, AUC: 0.937458\n","Epoch: 35, Test Accuracy: 0.894062, AUC: 0.943088\n","Epoch: 36, Test Accuracy: 0.896875, AUC: 0.943830\n","Epoch: 37, Test Accuracy: 0.869687, AUC: 0.944217\n","Early stopping...AUC has not improved for 20 rounds\n","Best AUC: %f 0.9447858997571509\n","0:00:44.119518\n"]}]},{"cell_type":"markdown","metadata":{"id":"FbeXSV2Pcchk"},"source":["### XGBoost"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJOxXfGGceWe","executionInfo":{"status":"ok","timestamp":1636459037741,"user_tz":-480,"elapsed":3372,"user":{"displayName":"Badly taught","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11394682269253795404"}},"outputId":"343c10ce-39e9-4bd5-d464-8c2d2e8464c0"},"source":["start = datetime.datetime.now()\n","xgbmodel_eth = XGBClassifier(objective='binary:logistic',  n_estimators = 1000, booster = 'gbtree', eval_metric = ['auc'])\n","xgbmodel_eth.fit(x_train_eth, y_train_eth, eval_set = [(x_val_eth, y_val_eth)], early_stopping_rounds = 10)\n","time_taken = datetime.datetime.now() - start\n","print(time_taken)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0]\tvalidation_0-auc:0.853198\n","Will train until validation_0-auc hasn't improved in 10 rounds.\n","[1]\tvalidation_0-auc:0.853335\n","[2]\tvalidation_0-auc:0.854387\n","[3]\tvalidation_0-auc:0.904049\n","[4]\tvalidation_0-auc:0.904055\n","[5]\tvalidation_0-auc:0.932984\n","[6]\tvalidation_0-auc:0.946811\n","[7]\tvalidation_0-auc:0.942837\n","[8]\tvalidation_0-auc:0.951333\n","[9]\tvalidation_0-auc:0.953319\n","[10]\tvalidation_0-auc:0.958422\n","[11]\tvalidation_0-auc:0.961091\n","[12]\tvalidation_0-auc:0.960159\n","[13]\tvalidation_0-auc:0.961581\n","[14]\tvalidation_0-auc:0.962737\n","[15]\tvalidation_0-auc:0.962447\n","[16]\tvalidation_0-auc:0.963646\n","[17]\tvalidation_0-auc:0.965628\n","[18]\tvalidation_0-auc:0.965874\n","[19]\tvalidation_0-auc:0.965832\n","[20]\tvalidation_0-auc:0.96535\n","[21]\tvalidation_0-auc:0.965797\n","[22]\tvalidation_0-auc:0.966874\n","[23]\tvalidation_0-auc:0.967594\n","[24]\tvalidation_0-auc:0.969821\n","[25]\tvalidation_0-auc:0.970315\n","[26]\tvalidation_0-auc:0.970392\n","[27]\tvalidation_0-auc:0.971323\n","[28]\tvalidation_0-auc:0.9712\n","[29]\tvalidation_0-auc:0.971984\n","[30]\tvalidation_0-auc:0.973294\n","[31]\tvalidation_0-auc:0.9738\n","[32]\tvalidation_0-auc:0.975137\n","[33]\tvalidation_0-auc:0.975639\n","[34]\tvalidation_0-auc:0.975987\n","[35]\tvalidation_0-auc:0.976954\n","[36]\tvalidation_0-auc:0.977374\n","[37]\tvalidation_0-auc:0.977792\n","[38]\tvalidation_0-auc:0.97809\n","[39]\tvalidation_0-auc:0.978638\n","[40]\tvalidation_0-auc:0.979323\n","[41]\tvalidation_0-auc:0.979427\n","[42]\tvalidation_0-auc:0.979686\n","[43]\tvalidation_0-auc:0.979679\n","[44]\tvalidation_0-auc:0.979975\n","[45]\tvalidation_0-auc:0.980027\n","[46]\tvalidation_0-auc:0.980222\n","[47]\tvalidation_0-auc:0.980339\n","[48]\tvalidation_0-auc:0.980381\n","[49]\tvalidation_0-auc:0.980702\n","[50]\tvalidation_0-auc:0.980895\n","[51]\tvalidation_0-auc:0.980942\n","[52]\tvalidation_0-auc:0.981166\n","[53]\tvalidation_0-auc:0.981154\n","[54]\tvalidation_0-auc:0.981131\n","[55]\tvalidation_0-auc:0.981356\n","[56]\tvalidation_0-auc:0.981217\n","[57]\tvalidation_0-auc:0.981367\n","[58]\tvalidation_0-auc:0.981471\n","[59]\tvalidation_0-auc:0.981689\n","[60]\tvalidation_0-auc:0.981784\n","[61]\tvalidation_0-auc:0.98181\n","[62]\tvalidation_0-auc:0.981849\n","[63]\tvalidation_0-auc:0.981749\n","[64]\tvalidation_0-auc:0.981958\n","[65]\tvalidation_0-auc:0.982072\n","[66]\tvalidation_0-auc:0.982313\n","[67]\tvalidation_0-auc:0.982564\n","[68]\tvalidation_0-auc:0.982505\n","[69]\tvalidation_0-auc:0.982517\n","[70]\tvalidation_0-auc:0.982627\n","[71]\tvalidation_0-auc:0.982942\n","[72]\tvalidation_0-auc:0.982932\n","[73]\tvalidation_0-auc:0.983019\n","[74]\tvalidation_0-auc:0.983097\n","[75]\tvalidation_0-auc:0.983156\n","[76]\tvalidation_0-auc:0.983381\n","[77]\tvalidation_0-auc:0.983516\n","[78]\tvalidation_0-auc:0.983609\n","[79]\tvalidation_0-auc:0.983693\n","[80]\tvalidation_0-auc:0.98383\n","[81]\tvalidation_0-auc:0.983923\n","[82]\tvalidation_0-auc:0.983918\n","[83]\tvalidation_0-auc:0.983878\n","[84]\tvalidation_0-auc:0.983913\n","[85]\tvalidation_0-auc:0.983993\n","[86]\tvalidation_0-auc:0.984133\n","[87]\tvalidation_0-auc:0.98423\n","[88]\tvalidation_0-auc:0.984252\n","[89]\tvalidation_0-auc:0.98431\n","[90]\tvalidation_0-auc:0.984264\n","[91]\tvalidation_0-auc:0.984173\n","[92]\tvalidation_0-auc:0.984296\n","[93]\tvalidation_0-auc:0.984527\n","[94]\tvalidation_0-auc:0.98455\n","[95]\tvalidation_0-auc:0.984623\n","[96]\tvalidation_0-auc:0.984846\n","[97]\tvalidation_0-auc:0.984947\n","[98]\tvalidation_0-auc:0.984955\n","[99]\tvalidation_0-auc:0.985001\n","[100]\tvalidation_0-auc:0.985252\n","[101]\tvalidation_0-auc:0.98534\n","[102]\tvalidation_0-auc:0.985364\n","[103]\tvalidation_0-auc:0.985427\n","[104]\tvalidation_0-auc:0.985407\n","[105]\tvalidation_0-auc:0.985382\n","[106]\tvalidation_0-auc:0.985429\n","[107]\tvalidation_0-auc:0.985549\n","[108]\tvalidation_0-auc:0.985601\n","[109]\tvalidation_0-auc:0.985553\n","[110]\tvalidation_0-auc:0.985694\n","[111]\tvalidation_0-auc:0.985774\n","[112]\tvalidation_0-auc:0.985835\n","[113]\tvalidation_0-auc:0.985922\n","[114]\tvalidation_0-auc:0.985971\n","[115]\tvalidation_0-auc:0.985905\n","[116]\tvalidation_0-auc:0.985973\n","[117]\tvalidation_0-auc:0.986141\n","[118]\tvalidation_0-auc:0.986244\n","[119]\tvalidation_0-auc:0.986316\n","[120]\tvalidation_0-auc:0.986525\n","[121]\tvalidation_0-auc:0.986557\n","[122]\tvalidation_0-auc:0.986618\n","[123]\tvalidation_0-auc:0.98663\n","[124]\tvalidation_0-auc:0.986663\n","[125]\tvalidation_0-auc:0.986681\n","[126]\tvalidation_0-auc:0.986867\n","[127]\tvalidation_0-auc:0.986936\n","[128]\tvalidation_0-auc:0.986949\n","[129]\tvalidation_0-auc:0.986981\n","[130]\tvalidation_0-auc:0.987058\n","[131]\tvalidation_0-auc:0.987115\n","[132]\tvalidation_0-auc:0.987183\n","[133]\tvalidation_0-auc:0.987167\n","[134]\tvalidation_0-auc:0.987257\n","[135]\tvalidation_0-auc:0.987306\n","[136]\tvalidation_0-auc:0.987358\n","[137]\tvalidation_0-auc:0.987426\n","[138]\tvalidation_0-auc:0.987422\n","[139]\tvalidation_0-auc:0.987422\n","[140]\tvalidation_0-auc:0.987421\n","[141]\tvalidation_0-auc:0.987426\n","[142]\tvalidation_0-auc:0.98747\n","[143]\tvalidation_0-auc:0.987564\n","[144]\tvalidation_0-auc:0.987617\n","[145]\tvalidation_0-auc:0.987681\n","[146]\tvalidation_0-auc:0.987687\n","[147]\tvalidation_0-auc:0.98768\n","[148]\tvalidation_0-auc:0.987708\n","[149]\tvalidation_0-auc:0.987731\n","[150]\tvalidation_0-auc:0.987734\n","[151]\tvalidation_0-auc:0.987733\n","[152]\tvalidation_0-auc:0.987732\n","[153]\tvalidation_0-auc:0.987732\n","[154]\tvalidation_0-auc:0.987735\n","[155]\tvalidation_0-auc:0.987735\n","[156]\tvalidation_0-auc:0.987734\n","[157]\tvalidation_0-auc:0.987734\n","[158]\tvalidation_0-auc:0.987734\n","[159]\tvalidation_0-auc:0.987734\n","[160]\tvalidation_0-auc:0.987733\n","[161]\tvalidation_0-auc:0.987732\n","[162]\tvalidation_0-auc:0.987732\n","[163]\tvalidation_0-auc:0.987732\n","[164]\tvalidation_0-auc:0.987731\n","Stopping. Best iteration:\n","[154]\tvalidation_0-auc:0.987735\n","\n","0:00:02.502346\n"]}]}]}